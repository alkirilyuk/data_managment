{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1: применяем PCA-трансформацию\n",
    "Модифицируйте файл train.py - добавьте в пайплайн обучения модели сжатие размерности до n_components=2 с помощью PCA и обучите модель в докере на \"сжатых\" данных. Сохраните полученный объект pca_transformer.pkl, который умеет выполнять сжатие данных.\n",
    "\n",
    "Решением домашки считается модифицированный файл train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:44:36,468 | INFO     | <ipython-input-1-427f6ae0:36   | Модель обучена и сохранена в C:\\Users\\Константин\\Desktop\\Алена\\jupyter\n",
      "2021-05-12 22:44:36,491 | INFO     | <ipython-input-1-427f6ae0:42   | psna_transformer  сохранен в файле ./data/pca_transformer.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "LOG_FORMAT = '%(asctime)s | %(levelname)-8s | %(filename)-25.25s:%(lineno)-4d | %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n",
    "# загрузка данных\n",
    "\n",
    "\n",
    "\n",
    "class TransformerPCA(object):\n",
    "    def __init__(self, source: Path):\n",
    "        self.data_source = np.genfromtxt(source, delimiter=',', skip_header=1)\n",
    "        self.X = self.data_source[:, :3]\n",
    "        self.y = self.data_source[:, 3]\n",
    "    \n",
    "\n",
    "    def PCA_transform(self):\n",
    "        pca_transformer = PCA(n_components=2).fit(self.X)\n",
    "        x_transformed = pca_transformer.transform(self.X)\n",
    "        return x_transformed\n",
    "\n",
    "    \n",
    "    def learn(self):\n",
    "        # обучение модели\n",
    "        clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "        clf.fit(self.X, self.y)\n",
    "        # сохраняем модель внутри контейнера в директории /www/classifier\n",
    "        path_to_save_cf = 'clf.pkl'\n",
    "        with open(path_to_save_cf, 'wb') as f:\n",
    "            pickle.dump(clf, f)\n",
    "            logging.info('Модель обучена и сохранена в %s' % Path().absolute())\n",
    "        # обучение модели на сжатых данных\n",
    "        clf.fit(self.PCA_transform(), self.y)\n",
    "        # Сохраняем модель обученную на сжатых данных\n",
    "        path_to_save_tf = './data/pca_transformer.pkl'\n",
    "        with open(path_to_save_tf, 'wb') as f:\n",
    "            logging.info(f'psna_transformer  сохранен в файле {path_to_save_tf}')\n",
    "            pickle.dump(clf, f)\n",
    "    \n",
    "    \n",
    "    def pipeline(self):\n",
    "        self.learn()\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path_to_data = 'data/client_segmentation.csv'\n",
    "pca_tf = TransformerPCA(path_to_data)\n",
    "pca_tf.pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание: трансформация входных фичей на лету\n",
    "Модифицируйте файл service.py: добавьте загрузку объекта для трансформации tsne_tansformer.pkl и применяйте её в докере для трансформации набора входных фич в сжатые:\n",
    "\n",
    "[x1, x2, x3] -> [x1_tsne, x2_tsne]\n",
    "Соответственно, predict надо выполнять на сжатых фичах\n",
    "\n",
    "Решением домашки считается модифицированный файл service.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:44:36,889 | INFO     | <ipython-input-2-bbd88b5c:73   | Загружаем обученные модели\n",
      "2021-05-12 22:44:36,897 | INFO     | <ipython-input-2-bbd88b5c:76   | Модель, обученная на сжатых данных загружена\n",
      "2021-05-12 22:44:36,897 | INFO     | <ipython-input-2-bbd88b5c:81   | Модель загружена\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Умеет выполнять классификацию клиентов по трём фичам\n",
    "Запускаем из python3:\n",
    "    python3 service.py\n",
    "Проверяем работоспособность:\n",
    "    curl http://127.0.0.1:5000/\n",
    "\"\"\"\n",
    "import json\n",
    "import http.server\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import socketserver\n",
    "import sys\n",
    "from http import HTTPStatus\n",
    "from re import compile\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# файл, куда посыпятся логи модели\n",
    "\n",
    "LOG_FORMAT = '%(asctime)s | %(levelname)-8s | %(filename)-25.25s:%(lineno)-4d | %(message)s'\n",
    "logging.basicConfig(filename=\"/www/classifier/data/service.log\", level=logging.INFO, format=LOG_FORMAT)\n",
    "\n",
    "\n",
    "def parse_params(params) -> dict:\n",
    "    \"\"\"\n",
    "        Выдираем параметры из GET-запроса\n",
    "    \"\"\"\n",
    "    params_list = params.split('&')\n",
    "    params_dict = {'x1': None, 'x2': None, 'x3': None}\n",
    "    for param in params_list:\n",
    "        key, value = param.split('=')\n",
    "        params_dict[key] = float(value)\n",
    "    return params_dict\n",
    "\n",
    "\n",
    "class Handler(http.server.SimpleHTTPRequestHandler):\n",
    "    \"\"\"Простой http-сервер\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def get_response(self) -> dict:\n",
    "        \"\"\"Пример запроса\n",
    "        \n",
    "        http://0.0.0.0:5000/classifier/?x1=1&x2=-2.2&x3=1.05\n",
    "        \"\"\"\n",
    "        response = {'ping': 'ok'}\n",
    "        params_parsed = self.path.split('?')\n",
    "        if len(params_parsed) == 2 and self.path.startswith('/classifier'):\n",
    "            params = params_parsed[1]\n",
    "            params_dict = parse_params(params)\n",
    "            response = params_dict\n",
    "            user_features = np.array([params_dict['x1'], params_dict['x2'], params_dict['x3']]).reshape(1, -1)\n",
    "            predicted_class = int(classifier_model.predict(user_features)[0])\n",
    "            logging.info('predicted_class %s' % predicted_class)\n",
    "            response.update({'predicted_class': predicted_class})\n",
    "        elif self.path.startswith('/ping/'):\n",
    "            response = {'message': 'pong'}\n",
    "\n",
    "        return response\n",
    "\n",
    "    def do_GET(self):\n",
    "        # заголовки ответа\n",
    "        self.send_response(HTTPStatus.OK)\n",
    "        self.send_header(\"Content-type\", \"application/json\")\n",
    "        self.end_headers()\n",
    "        self.wfile.write(json.dumps(self.get_response()).encode())\n",
    "\n",
    "\n",
    "logging.info('Загружаем обученные модели')\n",
    "with open('./data/pca_transformer.pkl', 'rb') as f:\n",
    "    transformer_model = pickle.load(f)\n",
    "    logging.info('Модель, обученная на сжатых данных загружена')\n",
    "\n",
    "path_to_save_cf = 'clf.pkl'\n",
    "with open(path_to_save_cf, 'rb') as f:\n",
    "    classifier_model = pickle.load(f)   \n",
    "    logging.info('Модель загружена')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classifier_service = socketserver.TCPServer(('', 5001), Handler)\n",
    "    classifier_service.serve_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание: Используем Flask\n",
    "Перепишите сервис на использование Flask. Вы можете взять готовый базовый образ с Flask, либо добавить установку в тот контейнер, который есть - это нужно сделать в Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging\n",
    "from flask import Flask, request, make_response\n",
    "\n",
    "app = Flask(__name__)\n",
    "LOG_FORMAT = '%(asctime)s | %(levelname)-8s | %(filename)-25.25s:%(lineno)-4d | %(message)s'\n",
    "logging.basicConfig(filename=\"/www/classifier/data/service.log\", level=logging.INFO, format=LOG_FORMAT)\n",
    "\n",
    "@app.route('/ping')\n",
    "def ping():\n",
    "    return {'message': 'pong'}\n",
    "\n",
    "@app.route('/predict')\n",
    "def predict():\n",
    "    args = request.args\n",
    "    try:\n",
    "        x1 = float(args['x1'])\n",
    "        x2 = float(args['x2'])\n",
    "        x3 = float(args['x3'])\n",
    "    except:\n",
    "        return {'msg': 'Wrong data'}\n",
    "    input_feautures = tf.transform([[x1, x2, x3]])\n",
    "    prediction = clf.predict(input_feautures)[0]\n",
    "    logging.info('predicted_class %s' % predicted_class)\n",
    "    response = make_response({'predicted_class': predicted_class})\n",
    "    return response\n",
    "\n",
    "with open('./data/pca_transformer.pkl', 'rb') as f:\n",
    "    tf = pickle.load(f)\n",
    "    logging.info('Модель, обученная на сжатых данных загружена')\n",
    "\n",
    "path_to_save_cf = 'clf.pkl'\n",
    "with open(path_to_save_cf, 'rb') as f:\n",
    "    clf = pickle.load(f)   \n",
    "    logging.info('Модель загружена')\n",
    "    \n",
    "#app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание: строим KNN\n",
    "В реальной жизни KNN-рекомендатель не стоит делать на основе sklearn.neighbors.NearestNeighbors - есть готовые реализации, заточенные специально для построения рекомендательных систем. Хорошим примером такой реализации является пакет implictit. В рамках домашней работы предлагается разобраться с реализацией KNN-рекомендателя из этой библиотеки\n",
    "\n",
    "Почитайте документацию по модулю implicit.nearest_neighbours.CosineRecommender. Обучите KNN-рекомендатель и воспользуйтесь методом recommend для построения рекомендаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "content_views = pd.read_csv(\n",
    "    'recsys_data/content_views.zip', delimiter=',', header=0, compression='zip',\n",
    "    names = ['user_id', 'content_id', 'view_duration', 'view_ts', 'dt', 'platform'],\n",
    "    dtype = {'user_id': np.uint32, 'content_id': np.uint16, 'view_duration': np.uint16},\n",
    "    parse_dates = [3, 4]\n",
    ")\n",
    "\n",
    "\n",
    "print('Количество просмотров %s' % content_views.user_id.count())\n",
    "\n",
    "content_views.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_description = pd.read_csv(\n",
    "    'recsys_data/content_description.zip', delimiter=',', header=0, compression='zip',\n",
    "    names = ['content_id', 'origin_country', 'release_date', 'kinopoisk_rating', 'compilation_id', 'genre'],\n",
    "    dtype = {'content_id': np.uint16},\n",
    "    parse_dates = [2]\n",
    ")\n",
    "\n",
    "print('Количество доступного контента %s' % content_description.content_id.count())\n",
    "\n",
    "content_description.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# кодируем индексы пользователей\n",
    "user_encoder = LabelEncoder()\n",
    "user_encoder.fit(content_views.user_id)\n",
    "\n",
    "# ереиндексация контента\n",
    "content_views = content_views.assign(\n",
    "    user_index = user_encoder.transform(content_views.user_id)\n",
    ")\n",
    "\n",
    "# кодируем индексы контента\n",
    "item_encoder = LabelEncoder()\n",
    "item_encoder.fit(content_views.content_id)\n",
    "\n",
    "# нова переиндексация\n",
    "content_views = content_views.assign(\n",
    "    item_index = item_encoder.transform(content_views.content_id)\n",
    ")\n",
    "\n",
    "\n",
    "content_views.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "num_users = content_views.user_index.max() + 1\n",
    "num_items = content_views.item_index.max() + 1\n",
    "num_interactions = content_views.shape[0]\n",
    "\n",
    "user_item = csr_matrix(\n",
    "    (np.ones(num_interactions),(content_views.user_index.values, content_views.item_index.values)),\n",
    "    shape=(num_users, num_items)\n",
    ")\n",
    "print('sparsity: %.4f' % (num_interactions / (num_users * num_items)))\n",
    "\n",
    "user_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids = train_test_split(\n",
    "    np.arange(start=0, stop=user_item.shape[0], step=1, dtype=np.uint32),\n",
    "    test_size=0.2\n",
    ")\n",
    "print(\n",
    "    \"Размер обучающей выборки %d пользователей, размер валидационной выборки %d пользователей\"\n",
    "    % (train_ids.size, test_ids.size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)\n",
    "\n",
    "# обучаемся только на тренировочной части пользователей\n",
    "model_knn.fit(user_item[train_ids,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColaborativeFilteringKNNRecommender:\n",
    "    def __init__(self, knn_model, user_item_matrix, num_neighbors):\n",
    "        self.knn_model = knn_model\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.top_recs = 50\n",
    "    \n",
    "    def make_recs(self, user_history: csr_matrix, top_recs: int):\n",
    "        neighbors = model_knn.kneighbors(\n",
    "            random_user_history,\n",
    "            self.num_neighbors,\n",
    "            return_distance=False\n",
    "        )[0]\n",
    "        full_recs = user_item[neighbors,:].max(axis=0)\n",
    "        # рекомендации - это то, что насмотрели ближайшие соседи\n",
    "        user_history_ids = user_history.nonzero()[1]\n",
    "        # последовательность id того контента, который смотрели ближайшие соседи\n",
    "        full_recs_ids = full_recs.nonzero()[1][:self.top_recs]\n",
    "        # исключаем из рекомендаций то, что уже было у упользователя в историии\n",
    "        success_recs = np.array([i for i in full_recs_ids if i in user_history_ids])\n",
    "        print(\"Число успешных рекомендаций %d из %d\" % (success_recs.size, top_recs))\n",
    "        \n",
    "        return np.array([i for i in full_recs_ids if i not in user_history_ids])[:10]\n",
    "\n",
    "\n",
    "# объект рекомендателя\n",
    "recommender = ColaborativeFilteringKNNRecommender(\n",
    "    knn_model=model_knn,\n",
    "    user_item_matrix=user_item,\n",
    "    num_neighbors=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример рекомендаций для случайного пользователя\n",
    "random_user_index = 934 #np.random.choice(test_ids)\n",
    "random_user_history = user_item.getrow(random_user_index).reshape(1, -1)\n",
    "\n",
    "recs = recommender.make_recs(random_user_history, top_recs=10)\n",
    "print('user_index %d, history: %s' % (random_user_index, random_user_history.nonzero()[1][:10]))\n",
    "print('recommendations: %s' % recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -- ВАШ КОД ТУТ --\n",
    "from implicit.nearest_neighbours import CosineRecommender\n",
    "# Настраиваем рекомендатель\n",
    "cr = CosineRecommender(K=50, num_threads=0)\n",
    "# Обучаем\n",
    "cr.fit(user_item[train_ids,  :])\n",
    "# Получаем рекоменлацию\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr.recommend(random_user_index, user_item[train_ids, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание: Item to Item\n",
    "Решите задачу c2c рекомендаций - вызовите метод similar_items для item_id=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr.similar_items(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание: обучаем Implicit\n",
    "Почитайте документацию по модулю implicit.als.AlternatingLeastSquares. Обучите ALS-рекомендатель и воспользуйтесь методом recommend для построения рекомендаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "recommender = AlternatingLeastSquares()\n",
    "recommender.fit(user_item[train_ids,:])\n",
    "\n",
    "recommender.recommend(random_user_index, user_item[train_ids,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание на метрики\n",
    "Даны два вектора - истинная история пользователя и объекты, которые считает релеватными ваша модель\n",
    "\n",
    "Вычислите\n",
    "\n",
    "precision\n",
    "recall\n",
    "precision@5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 0.1\n",
      "precision = 0.5\n",
      "precision@5 = 0.05183403900280742\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "user_interactions = [47315, 30004, 36322,  8942, 30820,  6086,  9126,   332, 16289,\n",
    "       39106, 39335, 48506, 48654,  9234, 29935,  2678, 36202, 22636, 18007, 39328, 15414, 30016, 35601,\n",
    "    58409, 21313,   386, 16303, 4397, 19644, 51887, 21659, 36325, 53030,  7764, 50266, 58734, 53419, 24121,\n",
    "    50806, 36092,  8868, 28037, 36131, 13561, 16298, 27508, 41722, 30189, 46490,  2676, 43328, 781, 48397,\n",
    "    41369, 39324, 36381, 39635, 27710, 47837, 28525, 12024, 56604, 41664, 37387, 48507, 413, 33526, 20059,\n",
    "    49781, 56648, 16283, 50805, 34254, 39325, 59374, 22620,  8865, 27512, 13875, 30011,  7621,\n",
    "    10544, 28076, 29716, 30054, 20490, 29466, 16852, 39363, 34250, 7024, 33541,   263, 21267, 25690, 23020,\n",
    "    41368, 53414,  2681, 30201] \n",
    "\n",
    "user_recs = [\n",
    "    50820, 27781, 36131, 50812, 36092, 12024, 59155, 30042, 15414, 19882, 21659, 27849, 39328, 34240, 2681,\n",
    "    21267, 50126, 58560, 7764, 49781\n",
    "]\n",
    "\n",
    "recall = len(set(user_interactions)&set(user_recs)) / len(set(user_interactions))\n",
    "precision = len(set(user_interactions)&set(user_recs)) / len(set(user_recs))\n",
    "print(f'recall = {recall}')\n",
    "print(f'precision = {precision}')\n",
    "# Вычисляем precision@5\n",
    "k = 5\n",
    "ap, i = 0, 0\n",
    "# Общий смысл оцениваем рекомендации по их месту в истории пользователя\n",
    "while i < len(user_recs):\n",
    "    if user_recs[i] in user_interactions:\n",
    "        index = user_interactions.index(user_recs[i])+1\n",
    "        ap += 1/index\n",
    "        k -= 1\n",
    "    i += 1\n",
    "        \n",
    "print(f'precision@5 = {ap*(1/5)}')     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
